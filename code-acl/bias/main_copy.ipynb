{"cells":[{"cell_type":"code","execution_count":1,"id":"S4LZFyZ6bB9d","metadata":{"executionInfo":{"elapsed":2223,"status":"ok","timestamp":1636580037046,"user":{"displayName":"Benjamin Basseri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02068950276609094857"},"user_tz":300},"id":"S4LZFyZ6bB9d"},"outputs":[],"source":["ENVIRONMENT = \"AWS\""]},{"cell_type":"code","execution_count":null,"id":"5e8a8c83","metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"code","execution_count":2,"id":"QUBi_CsJeOQe","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23207,"status":"ok","timestamp":1636580060248,"user":{"displayName":"Benjamin Basseri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02068950276609094857"},"user_tz":300},"id":"QUBi_CsJeOQe","outputId":"035242e9-b08f-412f-f2c7-f888cf739273"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Adds local modules to sys path for importing\n","import sys\n","import os\n","\n","# If running in colab, mount drive\n","if ENVIRONMENT == \"COLAB\":\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    os.chdir('/content/drive/MyDrive/CS105BProject/bias' )\n","    sys.path.append('/content/drive/MyDrive/CS105BProject')\n","elif ENVIRONMENT == \"AWS\":\n","    sys.path.append(\"~/fake-news-reasoning/code-acl/bias\")\n","sys.path.append(os.getcwd())\n"]},{"cell_type":"code","execution_count":3,"id":"om0_6fADizPI","metadata":{"executionInfo":{"elapsed":139,"status":"ok","timestamp":1636580125721,"user":{"displayName":"Benjamin Basseri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02068950276609094857"},"user_tz":300},"id":"om0_6fADizPI"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"id":"zR0Ws0UBd2IJ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":235475,"status":"ok","timestamp":1635266539333,"user":{"displayName":"Benjamin Basseri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02068950276609094857"},"user_tz":240},"id":"zR0Ws0UBd2IJ","outputId":"ab982abb-621c-47a7-a6c7-18fadd47f653"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data...\n","Data downloaded\n","Data unzipped\n"]},{"data":{"text/plain":[]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["%%bash\n","# Get data\n","# git clone --branch test --single-branch https://github.com/CS115-fake-news-detection/fake-news-reasoning.git\n","echo \"Downloading data...\"\n","wget -q https://www.dropbox.com/s/3v5oy3eddg3506j/multi_fc_publicdata.zip\n","echo \"Data downloaded\"\n","unzip -q multi_fc_publicdata.zip\n","rm multi_fc_publicdata.zip\n","echo \"Data unzipped\"\n","mv multi_fc_publicdata/ ../"]},{"cell_type":"code","execution_count":4,"id":"JS-Uo5DdnhH9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9068,"status":"ok","timestamp":1636580143590,"user":{"displayName":"Benjamin Basseri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02068950276609094857"},"user_tz":300},"id":"JS-Uo5DdnhH9","outputId":"06d5cc5c-746a-4809-e575-d194fbad66b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 8.0 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 8.6 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 71.2 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 84.5 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 71.2 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.3\n","Collecting pytorch-nlp\n","  Downloading pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n","\u001b[K     |████████████████████████████████| 90 kB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-nlp) (4.62.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-nlp) (1.19.5)\n","Installing collected packages: pytorch-nlp\n","Successfully installed pytorch-nlp-0.5.0\n"]}],"source":["!pip install transformers\n","!pip install pytorch-nlp"]},{"cell_type":"code","execution_count":8,"id":"eba4c237","metadata":{"executionInfo":{"elapsed":2045,"status":"ok","timestamp":1636580664606,"user":{"displayName":"Benjamin Basseri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02068950276609094857"},"user_tz":300},"id":"eba4c237"},"outputs":[],"source":["import sys\n","import os\n","# sys.path.append('../../code-acl')\n","# sys.path.append(os.getcwd())\n","sys.path.append('/content/drive/MyDrive/CS105BProject/bias/')\n","os.environ['OMP_NUM_THREADS'] = \"1\"\n","import argparse\n","import pandas as pd\n","import pickle\n","from model.generator import TransformerDataset, transformer_collate\n","from model.bertmodel import MyBertModel\n","from model.lstmmodel import LSTMModel\n","import torch\n","from parameters import BERT_MODEL_PATH, CLAIM_ONLY, CLAIM_AND_EVIDENCE, EVIDENCE_ONLY, DEVICE, INPUT_TYPE_ORDER\n","from transformers import AdamW\n","import numpy as np\n","from utils.utils import print_message, clean_str\n","from sklearn.metrics import f1_score\n","from sklearn.utils.class_weight import compute_class_weight\n","from collections import Counter\n","from torchnlp.word_to_vector import GloVe\n","from collections import Counter\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","#from hypopt import GridSearch\n","from model_selection import GridSearch\n","from tqdm import tqdm\n","\n","def load_data(dataset):\n","    #path = \"../../multi_fc_publicdata/\" + dataset + \"/\"\n","\n","    path = \"../multi_fc_publicdata/\" + dataset + \"/\"\n","\n","    main_data = pd.read_csv(path + dataset + \".tsv\", sep=\"\\t\", header=None)\n","    snippets_data = pd.read_csv(path + dataset + \"_snippets.tsv\", sep=\"\\t\", header=None)\n","    label_order = pickle.load(open(path + dataset + \"_labels.pkl\", \"rb\"))\n","    splits = pickle.load(open(path + dataset + \"_index_split.pkl\", \"rb\"))\n","\n","    return main_data, snippets_data, label_order, splits\n","\n","def make_generators(main_data, snippets_data, label_order, splits, params, dataset_generator=TransformerDataset, other_dataset=False):\n","    generators = []\n","\n","    all_labels = main_data.values[:,2]\n","    counter = Counter(all_labels)\n","    ss = \"\"\n","    for c in label_order:\n","        ss = ss + \", \" + str(c) + \" (\" + str(np.around(counter[c]/len(all_labels) * 100,1)) + \"\\%)\"\n","        #print(c, np.around(counter[c]/len(all_labels) * 100,1), \"%\", counter[c])\n","    print(\"len\", len(all_labels), ss)\n","\n","    for isplit, split in enumerate(splits):\n","        # print(f'isplit {isplit}')\n","        sub_main_data = main_data.values[split]\n","        # print(f'len sub_main_data: {len(sub_main_data)}')\n","        \n","        sub_snippets_data = snippets_data.values[split]\n","        # print(f'len sub_snippets_data: {len(sub_snippets_data)}')\n","\n","        \n","\n","        tmp = dataset_generator(sub_main_data, sub_snippets_data, label_order)\n","        if isplit == 0:\n","            generator = torch.utils.data.DataLoader(tmp, **params[0])\n","        else:\n","            generator = torch.utils.data.DataLoader(tmp, **params[1])\n","\n","        generators.append(generator)\n","\n","        # print(sub_main_data)\n","        # print(sub_snippets_data)\n","        # print(f'tmp: \\n {tmp[0]}')\n","        # gen0 = next(iter(generator))\n","        # print(f'gen0: \\n {gen0}')\n","\n","\n","    # make class weights\n","    labels = main_data.values[splits[0]][:,2]\n","    labels = np.array([label_order.index(v) for v in labels])\n","\n","\n","    if not other_dataset:\n","        label_weights = torch.tensor(compute_class_weight(\"balanced\", classes=np.arange(len(label_order)), y=labels).astype(np.float32))\n","    else:\n","        label_weights = None\n","\n","    return generators[0], generators[1], generators[2], label_weights\n","\n","def evaluate(generator, model, other_from=None, ignore_snippet=None):\n","    all_labels = []\n","    all_predictions = []\n","\n","    all_claimIDs = []\n","    all_logits = []\n","\n","    for vals in generator:\n","        claimIDs, claims, labels, snippets = vals[0], vals[1], vals[2], vals[3]\n","\n","        if ignore_snippet is not None:\n","            for i in range(len(snippets)):\n","                snippets[i][ignore_snippet] = \"filler\"\n","\n","        all_labels += labels\n","        logits = model(claims, snippets)\n","\n","        predictions = torch.argmax(logits, 1).cpu().numpy()\n","\n","        if other_from == \"pomt\": # other data is pomt, and model is trained on snes\n","            # this case is fine\n","            pass\n","        elif other_from == \"snes\": # other data is snes, and model is trained on pomt\n","            # in this case both \"pants on fire!\" and \"false\" should be considered as false\n","            predictions[predictions == 0] = 1 # 0 is \"pants on fire!\" and 1 is \"false\" for pomt.\n","\n","        all_predictions += predictions.tolist()\n","\n","        all_claimIDs += claimIDs\n","        all_logits += logits.cpu().numpy().tolist()\n","\n","    f1_micro = f1_score(all_labels, all_predictions, average=\"micro\")\n","    f1_macro = f1_score(all_labels, all_predictions, average=\"macro\")\n","\n","    return f1_micro, f1_macro, all_claimIDs, all_logits, all_labels, all_predictions\n","\n","def train_step(optimizer, vals, model, criterion):\n","    optimizer.zero_grad()\n","\n","    claimIDs, claims, labels, snippets = vals[0], vals[1], torch.tensor(vals[2]).to(DEVICE), vals[3]\n","\n","    logits = model(claims, snippets)\n","    loss = criterion(logits, labels)\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","    return loss\n","\n","\n","def get_embedding_matrix(generators, dataset, min_occurrence=1):\n","    savename = \"preprocessed/\" + dataset + \"_glove.pkl\"\n","    if os.path.exists(savename):\n","        tmp = pickle.load(open(savename, \"rb\"))\n","        glove_embedding_matrix = tmp[0]\n","        word2idx = tmp[1]\n","        idx2word = tmp[2]\n","        return glove_embedding_matrix, word2idx, idx2word\n","\n","    glove_vectors = GloVe('840B')\n","    all_claims = []\n","    all_snippets = []\n","    for gen in generators:\n","        for vals in gen:\n","            claims = vals[1]\n","            claims = [clean_str(v) for v in claims]\n","            snippets = vals[3]\n","            snippets = [clean_str(item) for sublist in snippets for item in sublist]\n","\n","            all_claims += claims\n","            all_snippets += snippets\n","\n","    all_words = [word for v in all_claims+all_snippets for word in v.split(\" \")]\n","    counter = Counter(all_words)\n","    all_words = set(all_words)\n","    all_words = list(set([word for word in all_words if counter[word] > min_occurrence]))\n","    word2idx = {word: i+2 for i, word in enumerate(all_words)} # reserve 0 for potential mask and 1 for unk token\n","    idx2word = {word2idx[key]: key for key in word2idx}\n","\n","    num_words = len(idx2word)\n","\n","    glove_embedding_matrix = np.random.random((num_words+2, 300)) - 0.5\n","    missed = 0\n","    for word in word2idx:\n","        if word in glove_vectors:\n","            glove_embedding_matrix[word2idx[word]] = glove_vectors[word]\n","        else:\n","            missed += 1\n","\n","    pickle.dump([glove_embedding_matrix, word2idx, idx2word], open(savename, \"wb\"))\n","    return glove_embedding_matrix, word2idx, idx2word\n","\n","def train_model(model, criterion, optimizer, train_generator, val_generator, test_generator, args, other_generator, savename):\n","    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    params = sum([np.prod(p.size()) for p in model_parameters])\n","    print(\"model parameters\", params)\n","\n","    num_epochs = 0\n","    patience_counter = 0\n","    patience_max = 10\n","    best_f1 = -np.inf\n","    while (True):\n","        train_losses = []\n","\n","        model.train()\n","        for ivals, vals in enumerate(train_generator):\n","            loss = train_step(optimizer, vals, model, criterion)\n","            train_losses.append(loss.item())\n","\n","        num_epochs += 1\n","        print_message(\"TRAIN loss\", np.mean(train_losses), num_epochs)\n","\n","        if num_epochs % args.eval_per_epoch == 0:\n","            model.eval()\n","            with torch.no_grad():\n","                val_f1micro, val_f1macro, val_claimIDs, val_logits, val_labels, val_predictions = evaluate(val_generator, model)\n","                print_message(\"VALIDATION F1micro, F1macro, loss:\", val_f1micro, val_f1macro, len(val_claimIDs))\n","\n","            if val_f1macro > best_f1:\n","                with torch.no_grad():\n","                    test_f1micro, test_f1macro, test_claimIDs, test_logits, test_labels, test_predictions = evaluate(test_generator, model)\n","                    print_message(\"TEST F1micro, F1macro, loss:\", test_f1micro, test_f1macro, len(test_claimIDs))\n","\n","                    other_test_f1micro, other_test_f1macro, other_test_claimIDs, other_test_logits, other_test_labels, other_test_predictions = evaluate(other_generator, model, other_from=\"snes\" if args.dataset == \"pomt\" else \"pomt\")\n","                    print_message(\"OTHER-TEST F1micro, F1macro, loss:\", other_test_f1micro, other_test_f1macro, len(other_test_claimIDs))\n","\n","                    test_remove_top_bottom = []\n","                    test_remove_bottom_top = []\n","                    other_test_remove_top_bottom = []\n","                    other_test_remove_bottom_top = []\n","                    ten = np.arange(10)\n","                    if args.inputtype != \"CLAIM_ONLY\":\n","                        for i in tqdm(range(10)):\n","                            top_is = ten[:(i+1)]\n","                            bottom_is = ten[-(i+1):]\n","                            test_remove_top_bottom.append( evaluate(test_generator, model, ignore_snippet=top_is) )\n","                            test_remove_bottom_top.append( evaluate(test_generator, model, ignore_snippet=bottom_is) )\n","                            other_test_remove_top_bottom.append(evaluate(other_generator, model, other_from=\"snes\" if args.dataset == \"pomt\" else \"pomt\", ignore_snippet=top_is))\n","                            other_test_remove_bottom_top.append(evaluate(other_generator, model, other_from=\"snes\" if args.dataset == \"pomt\" else \"pomt\", ignore_snippet=bottom_is))\n","\n","                        print_message([np.around(v[1], 4) for v in test_remove_top_bottom])\n","                        print_message([np.around(v[1], 4) for v in test_remove_bottom_top])\n","                        print_message([np.around(v[1], 4) for v in other_test_remove_top_bottom])\n","                        print_message([np.around(v[1], 4) for v in other_test_remove_bottom_top])\n","\n","                patience_counter = 0\n","                best_f1 = val_f1macro\n","                val_store = [val_f1micro, val_f1macro, val_claimIDs, val_logits, val_labels, val_predictions]\n","                test_store = [test_f1micro, test_f1macro, test_claimIDs, test_logits, test_labels, test_predictions, test_remove_top_bottom, test_remove_bottom_top]\n","                other_test_store = [other_test_f1micro, other_test_f1macro, other_test_claimIDs, other_test_logits, other_test_labels, other_test_predictions, other_test_remove_top_bottom, other_test_remove_bottom_top]\n","                misc_store = [args]\n","                total_store = [val_store, test_store, other_test_store, misc_store]\n","            else:\n","                patience_counter += 1\n","\n","            print_message(\"PATIENCE\", patience_counter, \"/\", patience_max)\n","\n","            if patience_counter >= patience_max:\n","                pickle.dump(total_store, open(savename, \"wb\"))\n","                break\n","\n","def run_bert(args, train_generator, val_generator, test_generator, label_weights, inputtype, label_order, savename, other_generator):\n","    model = MyBertModel.from_pretrained(BERT_MODEL_PATH, labelnum=len(label_order), input_type=inputtype)\n","    model.to(DEVICE)\n","    print(\"Model has been put on the torch device...\")\n","\n","    criterion = torch.nn.CrossEntropyLoss(weight=label_weights.to(DEVICE))\n","    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, eps=1e-8)\n","    optimizer.zero_grad()\n","\n","    train_model(model, criterion, optimizer, train_generator, val_generator, test_generator, args, other_generator, savename)\n","\n","def run_lstm(args, train_generator, val_generator, test_generator, label_weights, inputtype, label_order, savename, other_generator):\n","    glove_embedding_matrix, word2idx, idx2word = get_embedding_matrix([train_generator, val_generator, test_generator, other_generator], args.dataset)\n","\n","    model = LSTMModel(args.lstm_hidden_dim, args.lstm_layers, args.lstm_dropout, len(label_order), word2idx, glove_embedding_matrix, input_type=inputtype)\n","    model.to(DEVICE)\n","\n","    criterion = torch.nn.CrossEntropyLoss(weight=label_weights.to(DEVICE))\n","    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, eps=1e-8)\n","    optimizer.zero_grad()\n","\n","    train_model(model, criterion, optimizer, train_generator, val_generator, test_generator, args, other_generator, savename)\n","\n","    return model\n","\n","def filter_snippet_for_bow(generator, ignore_snippet, inputtype):\n","    samples = []\n","    for vals in generator:\n","        claims = vals[1]\n","        labels = vals[2]\n","        snippets = vals[3]\n","\n","        for i in range(len(snippets)):\n","            snippets[i][ignore_snippet] = \"filler\"\n","\n","        for i in range(len(claims)):\n","            if inputtype == CLAIM_AND_EVIDENCE:\n","                sample = clean_str(claims[i]) + \" \".join([clean_str(v) for v in snippets[i]])\n","            elif inputtype == CLAIM_ONLY:\n","                sample = clean_str(claims[i])\n","            elif inputtype == EVIDENCE_ONLY:\n","                sample = \" \".join([clean_str(v) for v in snippets[i]])\n","            else:\n","                raise Exception(\"Unknown type\", inputtype)\n","            samples.append(sample)\n","    return samples\n","\n","def get_bows_labels(generators, dataset, inputtype):\n","    all_samples = []\n","    all_labels = []\n","\n","    for gen in generators:\n","        gen_samples = []\n","        gen_labels = []\n","        for vals in gen:\n","            claims = vals[1]\n","            labels = vals[2]\n","            snippets = vals[3]\n","\n","            for i in range(len(claims)):\n","                if inputtype == CLAIM_AND_EVIDENCE:\n","                    sample = clean_str(claims[i]) + \" \".join([clean_str(v) for v in snippets[i]])\n","                elif inputtype == CLAIM_ONLY:\n","                    sample = clean_str(claims[i])\n","                elif inputtype == EVIDENCE_ONLY:\n","                    sample = \" \".join([clean_str(v) for v in snippets[i]])\n","                else:\n","                    raise Exception(\"Unknown type\", inputtype)\n","                gen_samples.append(sample)\n","                gen_labels.append(labels[i])\n","\n","        all_samples.append(gen_samples)\n","        all_labels.append(gen_labels)\n","\n","    test_remove_top_bottom = []\n","    test_remove_bottom_top = []\n","    other_test_remove_top_bottom = []\n","    other_test_remove_bottom_top = []\n","    ten = np.arange(10)\n","    for i in tqdm(range(10)):\n","        top_is = ten[:(i + 1)]\n","        bottom_is = ten[-(i + 1):]\n","        test_remove_top_bottom.append( filter_snippet_for_bow(generators[-2], top_is, inputtype) )\n","        test_remove_bottom_top.append( filter_snippet_for_bow(generators[-2], bottom_is, inputtype) )\n","        other_test_remove_top_bottom.append( filter_snippet_for_bow(generators[-1], top_is, inputtype) )\n","        other_test_remove_bottom_top.append( filter_snippet_for_bow(generators[-1], bottom_is, inputtype) )\n","\n","    vectorizer = TfidfVectorizer(min_df=2)\n","    vectorizer.fit([item for sublist in all_samples for item in sublist])\n","\n","    bows = [vectorizer.transform(all_samples[i]) for i in range(len(all_samples))]\n","\n","    test_remove_top_bottom = [vectorizer.transform(test_remove_top_bottom[i]) for i in range(len(test_remove_top_bottom))]\n","    test_remove_bottom_top = [vectorizer.transform(test_remove_bottom_top[i]) for i in range(len(test_remove_bottom_top))]\n","    other_test_remove_top_bottom = [vectorizer.transform(other_test_remove_top_bottom[i]) for i in range(len(other_test_remove_top_bottom))]\n","    other_test_remove_bottom_top = [vectorizer.transform(other_test_remove_bottom_top[i]) for i in range(len(other_test_remove_bottom_top))]\n","\n","    return bows, all_labels, test_remove_top_bottom, test_remove_bottom_top, other_test_remove_top_bottom, other_test_remove_bottom_top\n","\n","def run_bow(args, train_generator, val_generator, test_generator, label_weights, inputtype, label_order, savename, other_test_generator):\n","    # print(f'train_generator0 :\\n {next(iter(train_generator))}')\n","\n","    bows, labels, test_remove_top_bottom, test_remove_bottom_top, other_test_remove_top_bottom, other_test_remove_bottom_top = get_bows_labels([train_generator, val_generator, test_generator, other_test_generator], args.dataset, inputtype)\n","\n","    train_bow, val_bow, test_bow, other_test_bow = bows[0], bows[1], bows[2], bows[3]\n","    train_labels, val_labels, test_labels, other_test_labels = labels[0], labels[1], labels[2], labels[3]\n","\n","    label_weights = label_weights.numpy()\n","    weights = {i: label_weights[i] for i in range(len(label_weights))}\n","\n","    # print(f'****** run bow train_bow \\n {train_bow}')\n","    # print('*********')\n","\n","    param_grid = [\n","        {'n_estimators': [100, 500, 1000], 'min_samples_leaf': [1, 3, 5, 10], 'min_samples_split': [2, 5, 10]}\n","    ]\n","\n","    opt = GridSearch(model=RandomForestClassifier(n_jobs=5, class_weight=weights), param_grid=param_grid, parallelize=False)\n","\n","    \n","    opt.fit(train_bow, train_labels, val_bow, val_labels, scoring=\"f1_macro\")\n","\n","    def rf_eval(model, bow, labels, other_from=None):\n","        preds = model.predict(bow)\n","\n","        if other_from == \"pomt\": # other data is pomt, and model is trained on snes\n","            # this case is fine\n","            pass\n","        elif other_from == \"snes\": # other data is snes, and model is trained on pomt\n","            # in this case both \"pants on fire!\" and \"false\" should be considered as false\n","            preds[preds == 0] = 1 # 0 is \"pants on fire!\" and 1 is \"false\" for pomt.\n","\n","        f1_macro = f1_score(labels, preds, average=\"macro\")\n","        f1_micro = f1_score(labels, preds, average=\"micro\")\n","        return f1_micro, f1_macro, labels, preds\n","\n","    # val_store = [val_f1micro, val_f1macro, val_claimIDs, val_logits, val_labels, val_predictions]\n","    # test_store = [test_f1micro, test_f1macro, test_claimIDs, test_logits, test_labels, test_predictions,test_remove_top_bottom, test_remove_bottom_top]\n","    # other_test_store = [other_test_f1micro, other_test_f1macro, other_test_claimIDs, other_test_logits,\n","    #                     other_test_labels, other_test_predictions, other_test_remove_top_bottom,\n","    #                     other_test_remove_bottom_top]\n","    #misc_store = [args]\n","\n","\n","    val_store = rf_eval(opt, val_bow, val_labels)\n","    test_store = list(rf_eval(opt, test_bow, test_labels)) + [[rf_eval(opt, test_remove_top_bottom[i], test_labels) for i in range(10)],\n","                                                       [rf_eval(opt, test_remove_bottom_top[i], test_labels) for i in range(10)]]\n","    other_test_store = list(rf_eval(opt, other_test_bow, other_test_labels, other_from=\"snes\" if args.dataset == \"pomt\" else \"pomt\")) + [[rf_eval(opt, other_test_remove_top_bottom[i], other_test_labels, other_from=\"snes\" if args.dataset == \"pomt\" else \"pomt\") for i in range(10)],\n","                                                       [rf_eval(opt, other_test_remove_bottom_top[i], other_test_labels, other_from=\"snes\" if args.dataset == \"pomt\" else \"pomt\") for i in range(10)]]\n","    misc_store = [opt.get_best_params()]\n","    total_store = [val_store, test_store, other_test_store, misc_store]\n","\n","    print_message(\"VALIDATION\", val_store[0], val_store[1])\n","    print_message(\"TEST\", test_store[0], test_store[1])\n","    print_message(\"OTHER-TEST\", other_test_store[0], other_test_store[1])\n","\n","    print_message([np.around(v[1], 4) for v in test_store[-2]])\n","    print_message([np.around(v[1], 4) for v in test_store[-1]])\n","    print_message([np.around(v[1], 4) for v in other_test_store[-2]])\n","    print_message([np.around(v[1], 4) for v in other_test_store[-1]])\n","    print(misc_store)\n","\n","    pickle.dump(total_store, open(savename, \"wb\"))\n","\n","def filter_websites(snippets_data):\n","    bad_websites = [\"factcheck.org\", \"politifact.com\", \"snopes.com\", \"fullfact.org\", \"factscan.ca\"]\n","    ids = snippets_data.values[:, 0]\n","    remove_count = 0\n","    for i, id in enumerate(ids):\n","        with open(\"../../multi_fc_publicdata/snippets/\" + id, \"r\", encoding=\"utf-8\") as f:\n","            lines = f.readlines()\n","\n","        links = [line.strip().split(\"\\t\")[-1] for line in lines]\n","        remove = [False for _ in range(10)]\n","        for j in range(len(links)):\n","            remove[j] = any([bad in links[j] for bad in bad_websites])\n","        remove = remove[:10]  # 1 data sample has 11 links by mistake in the dataset\n","        snippets_data.iloc[i, [False] + remove] = \"filler\"\n","\n","        remove_count += np.sum(remove)\n","    print_message(\"REMOVE COUNT\", remove_count)\n","    return snippets_data\n","\n"]},{"cell_type":"code","execution_count":9,"id":"5609720e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2640967,"status":"ok","timestamp":1636583367043,"user":{"displayName":"Benjamin Basseri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02068950276609094857"},"user_tz":300},"id":"5609720e","outputId":"94183a49-c9df-49c1-a0f1-e5d7f07e86c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["len 5069 , false (64.3\\%), mostly false (7.5\\%), mixture (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","len 13581 , false (29.7\\%), mostly false (17.0\\%), mixture (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","model parameters 1678089\n","[Nov 10, 21:45:49] TRAIN loss 1.6047742039233714 1\n","[Nov 10, 21:45:50] VALIDATION F1micro, F1macro, loss: 0.6429980276134122 0.1565426170468187 507\n","[Nov 10, 21:45:54] TEST F1micro, F1macro, loss: 0.6429980276134122 0.1565426170468187 1014\n","[Nov 10, 21:46:04] OTHER-TEST F1micro, F1macro, loss: 0.29738682370261316 0.09168794326241134 2717\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10/10 [03:08<00:00, 18.85s/it]"]},{"name":"stdout","output_type":"stream","text":["[Nov 10, 21:49:12] [0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565]\n","[Nov 10, 21:49:12] [0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565]\n","[Nov 10, 21:49:12] [0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917]\n","[Nov 10, 21:49:12] [0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917]\n","[Nov 10, 21:49:12] PATIENCE 0 / 10\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Nov 10, 21:49:33] TRAIN loss 1.6014498187614992 2\n","[Nov 10, 21:49:34] VALIDATION F1micro, F1macro, loss: 0.6272189349112426 0.1694253011326182 507\n","[Nov 10, 21:49:38] TEST F1micro, F1macro, loss: 0.6370808678500987 0.17419002796535027 1014\n","[Nov 10, 21:49:47] OTHER-TEST F1micro, F1macro, loss: 0.29628266470371734 0.1336480398045242 2717\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10/10 [03:07<00:00, 18.78s/it]"]},{"name":"stdout","output_type":"stream","text":["[Nov 10, 21:52:55] [0.1564, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1691, 0.1801]\n","[Nov 10, 21:52:55] [0.1742, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1801]\n","[Nov 10, 21:52:55] [0.1033, 0.0932, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0918, 0.1319]\n","[Nov 10, 21:52:55] [0.114, 0.0926, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.1319]\n","[Nov 10, 21:52:55] PATIENCE 0 / 10\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Nov 10, 21:53:16] TRAIN loss 1.600072490202414 3\n","[Nov 10, 21:53:17] VALIDATION F1micro, F1macro, loss: 0.5877712031558185 0.19531171442936152 507\n","[Nov 10, 21:53:21] TEST F1micro, F1macro, loss: 0.6025641025641025 0.18627007494718495 1014\n","[Nov 10, 21:53:30] OTHER-TEST F1micro, F1macro, loss: 0.2885535517114464 0.14338706308229868 2717\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10/10 [03:07<00:00, 18.76s/it]"]},{"name":"stdout","output_type":"stream","text":["[Nov 10, 21:56:37] [0.1567, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.188, 0.1995]\n","[Nov 10, 21:56:37] [0.1859, 0.1592, 0.1566, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1995]\n","[Nov 10, 21:56:37] [0.115, 0.0941, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0918, 0.1459]\n","[Nov 10, 21:56:37] [0.1331, 0.0979, 0.0916, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.1459]\n","[Nov 10, 21:56:37] PATIENCE 0 / 10\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Nov 10, 21:56:58] TRAIN loss 1.596267115962398 4\n","[Nov 10, 21:57:00] VALIDATION F1micro, F1macro, loss: 0.41025641025641024 0.2050240824393747 507\n","[Nov 10, 21:57:03] TEST F1micro, F1macro, loss: 0.41420118343195267 0.19104199632888158 1014\n","[Nov 10, 21:57:13] OTHER-TEST F1micro, F1macro, loss: 0.25874125874125875 0.161902414786437 2717\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10/10 [03:08<00:00, 18.82s/it]"]},{"name":"stdout","output_type":"stream","text":["[Nov 10, 22:00:21] [0.2156, 0.1801, 0.1563, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1828, 0.1874]\n","[Nov 10, 22:00:21] [0.1938, 0.2141, 0.1804, 0.1625, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1874]\n","[Nov 10, 22:00:21] [0.1713, 0.1361, 0.0988, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0918, 0.1421]\n","[Nov 10, 22:00:21] [0.1704, 0.1538, 0.1151, 0.0939, 0.0916, 0.0917, 0.0917, 0.0917, 0.0917, 0.1421]\n","[Nov 10, 22:00:21] PATIENCE 0 / 10\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Nov 10, 22:00:42] TRAIN loss 1.5953730989146877 5\n","[Nov 10, 22:00:43] VALIDATION F1micro, F1macro, loss: 0.5641025641025641 0.19591446389982947 507\n","[Nov 10, 22:00:43] PATIENCE 1 / 10\n","[Nov 10, 22:01:04] TRAIN loss 1.5883691912298803 6\n","[Nov 10, 22:01:06] VALIDATION F1micro, F1macro, loss: 0.4378698224852071 0.18619379429456256 507\n","[Nov 10, 22:01:06] PATIENCE 2 / 10\n","[Nov 10, 22:01:27] TRAIN loss 1.5866524641578261 7\n","[Nov 10, 22:01:29] VALIDATION F1micro, F1macro, loss: 0.46942800788954636 0.20951867713342426 507\n","[Nov 10, 22:01:32] TEST F1micro, F1macro, loss: 0.46844181459566075 0.2198495703274775 1014\n","[Nov 10, 22:01:41] OTHER-TEST F1micro, F1macro, loss: 0.25322046374677953 0.17793786954418278 2717\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10/10 [03:07<00:00, 18.76s/it]"]},{"name":"stdout","output_type":"stream","text":["[Nov 10, 22:04:49] [0.2141, 0.1919, 0.1654, 0.1563, 0.1565, 0.1565, 0.1565, 0.1565, 0.1783, 0.1662]\n","[Nov 10, 22:04:49] [0.2206, 0.2063, 0.1855, 0.1784, 0.1652, 0.1658, 0.1564, 0.1565, 0.1565, 0.1662]\n","[Nov 10, 22:04:49] [0.1742, 0.1417, 0.1042, 0.0946, 0.0917, 0.0917, 0.0917, 0.0917, 0.0918, 0.1503]\n","[Nov 10, 22:04:49] [0.1761, 0.1647, 0.1235, 0.1015, 0.093, 0.0916, 0.0917, 0.0917, 0.0917, 0.1503]\n","[Nov 10, 22:04:49] PATIENCE 0 / 10\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Nov 10, 22:05:10] TRAIN loss 1.5768557496972986 8\n","[Nov 10, 22:05:12] VALIDATION F1micro, F1macro, loss: 0.4891518737672584 0.22096023865031036 507\n","[Nov 10, 22:05:15] TEST F1micro, F1macro, loss: 0.4960552268244576 0.24259108085323397 1014\n","[Nov 10, 22:05:24] OTHER-TEST F1micro, F1macro, loss: 0.26720647773279355 0.19138346956509092 2717\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10/10 [03:07<00:00, 18.76s/it]"]},{"name":"stdout","output_type":"stream","text":["[Nov 10, 22:08:32] [0.2255, 0.2135, 0.1929, 0.1683, 0.1594, 0.1562, 0.1564, 0.1562, 0.163, 0.1007]\n","[Nov 10, 22:08:32] [0.2377, 0.2263, 0.2076, 0.1982, 0.1842, 0.1725, 0.1677, 0.159, 0.1557, 0.1007]\n","[Nov 10, 22:08:32] [0.1757, 0.1554, 0.1195, 0.0986, 0.0955, 0.0925, 0.0917, 0.0917, 0.0925, 0.1695]\n","[Nov 10, 22:08:32] [0.1836, 0.1685, 0.1448, 0.1107, 0.0987, 0.0952, 0.0923, 0.0926, 0.0933, 0.1695]\n","[Nov 10, 22:08:32] PATIENCE 0 / 10\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Nov 10, 22:08:53] TRAIN loss 1.5727000768120225 9\n","[Nov 10, 22:08:54] VALIDATION F1micro, F1macro, loss: 0.4911242603550296 0.23250743969332702 507\n","[Nov 10, 22:08:58] TEST F1micro, F1macro, loss: 0.4635108481262328 0.21430170591324518 1014\n","[Nov 10, 22:09:07] OTHER-TEST F1micro, F1macro, loss: 0.24843577475156423 0.16909741032298098 2717\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10/10 [03:07<00:00, 18.75s/it]"]},{"name":"stdout","output_type":"stream","text":["[Nov 10, 22:12:15] [0.2146, 0.2136, 0.1995, 0.1657, 0.1596, 0.1565, 0.1565, 0.1562, 0.1487, 0.0472]\n","[Nov 10, 22:12:15] [0.2131, 0.2086, 0.2116, 0.2036, 0.1783, 0.1689, 0.1564, 0.1564, 0.1562, 0.0472]\n","[Nov 10, 22:12:15] [0.1663, 0.152, 0.124, 0.0994, 0.0948, 0.0917, 0.0917, 0.0917, 0.0936, 0.0706]\n","[Nov 10, 22:12:15] [0.1653, 0.166, 0.1433, 0.1104, 0.0964, 0.0924, 0.0917, 0.0917, 0.0917, 0.0706]\n","[Nov 10, 22:12:15] PATIENCE 0 / 10\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Nov 10, 22:12:36] TRAIN loss 1.5666737964561395 10\n","[Nov 10, 22:12:37] VALIDATION F1micro, F1macro, loss: 0.5167652859960552 0.23396341810320304 507\n","[Nov 10, 22:12:41] TEST F1micro, F1macro, loss: 0.4990138067061144 0.2192531551518421 1014\n","[Nov 10, 22:12:50] OTHER-TEST F1micro, F1macro, loss: 0.26720647773279355 0.18145989133212745 2717\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10/10 [03:08<00:00, 18.82s/it]"]},{"name":"stdout","output_type":"stream","text":["[Nov 10, 22:15:58] [0.215, 0.2097, 0.1734, 0.1597, 0.1565, 0.1565, 0.1565, 0.1565, 0.1496, 0.0416]\n","[Nov 10, 22:15:58] [0.2203, 0.2148, 0.2062, 0.1779, 0.1658, 0.1596, 0.1564, 0.1565, 0.1564, 0.0416]\n","[Nov 10, 22:15:58] [0.1663, 0.1438, 0.1134, 0.0965, 0.0935, 0.0918, 0.0918, 0.0917, 0.0941, 0.066]\n","[Nov 10, 22:15:58] [0.1745, 0.1669, 0.1277, 0.0996, 0.0933, 0.0925, 0.0916, 0.0917, 0.0917, 0.066]\n","[Nov 10, 22:15:58] PATIENCE 0 / 10\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Nov 10, 22:16:19] TRAIN loss 1.5596770944896046 11\n","[Nov 10, 22:16:21] VALIDATION F1micro, F1macro, loss: 0.40236686390532544 0.2150121328727717 507\n","[Nov 10, 22:16:21] PATIENCE 1 / 10\n","[Nov 10, 22:16:42] TRAIN loss 1.551466575613967 12\n","[Nov 10, 22:16:43] VALIDATION F1micro, F1macro, loss: 0.4161735700197239 0.22452309438835893 507\n","[Nov 10, 22:16:43] PATIENCE 2 / 10\n","[Nov 10, 22:17:04] TRAIN loss 1.5449568027848597 13\n","[Nov 10, 22:17:06] VALIDATION F1micro, F1macro, loss: 0.46745562130177515 0.24534613845538217 507\n","[Nov 10, 22:17:09] TEST F1micro, F1macro, loss: 0.46055226824457596 0.24426055803448188 1014\n","[Nov 10, 22:17:19] OTHER-TEST F1micro, F1macro, loss: 0.2528524107471476 0.21689772045671357 2717\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10/10 [03:07<00:00, 18.73s/it]"]},{"name":"stdout","output_type":"stream","text":["[Nov 10, 22:20:26] [0.2351, 0.2134, 0.2014, 0.1939, 0.1964, 0.1944, 0.1969, 0.1512, 0.078, 0.0279]\n","[Nov 10, 22:20:26] [0.2412, 0.2331, 0.2122, 0.2117, 0.2157, 0.2184, 0.2155, 0.1969, 0.1356, 0.0279]\n","[Nov 10, 22:20:26] [0.2008, 0.1735, 0.1571, 0.145, 0.1316, 0.1242, 0.1339, 0.157, 0.1573, 0.058]\n","[Nov 10, 22:20:26] [0.2117, 0.1884, 0.1621, 0.1508, 0.1362, 0.1338, 0.1242, 0.1426, 0.16, 0.058]\n","[Nov 10, 22:20:26] PATIENCE 0 / 10\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Nov 10, 22:20:47] TRAIN loss 1.5436873929994601 14\n","[Nov 10, 22:20:49] VALIDATION F1micro, F1macro, loss: 0.42406311637080873 0.23169454641604564 507\n","[Nov 10, 22:20:49] PATIENCE 1 / 10\n","[Nov 10, 22:21:10] TRAIN loss 1.5383128765467051 15\n","[Nov 10, 22:21:11] VALIDATION F1micro, F1macro, loss: 0.40828402366863903 0.23096778815874516 507\n","[Nov 10, 22:21:11] PATIENCE 2 / 10\n","[Nov 10, 22:21:33] TRAIN loss 1.5212226939630937 16\n","[Nov 10, 22:21:34] VALIDATION F1micro, F1macro, loss: 0.3431952662721893 0.1977870696400626 507\n","[Nov 10, 22:21:34] PATIENCE 3 / 10\n","[Nov 10, 22:21:55] TRAIN loss 1.514678061545432 17\n","[Nov 10, 22:21:57] VALIDATION F1micro, F1macro, loss: 0.3905325443786982 0.22951326574349123 507\n","[Nov 10, 22:21:57] PATIENCE 4 / 10\n","[Nov 10, 22:22:18] TRAIN loss 1.507170110135465 18\n","[Nov 10, 22:22:19] VALIDATION F1micro, F1macro, loss: 0.5226824457593688 0.2661521275876955 507\n","[Nov 10, 22:22:23] TEST F1micro, F1macro, loss: 0.5167652859960552 0.260544850028564 1014\n","[Nov 10, 22:22:32] OTHER-TEST F1micro, F1macro, loss: 0.2719911667280088 0.19954314119836686 2717\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10/10 [03:07<00:00, 18.74s/it]"]},{"name":"stdout","output_type":"stream","text":["[Nov 10, 22:25:40] [0.218, 0.1906, 0.1844, 0.1816, 0.1767, 0.1713, 0.1331, 0.048, 0.028, 0.0279]\n","[Nov 10, 22:25:40] [0.2591, 0.2167, 0.1917, 0.1818, 0.1822, 0.1786, 0.1637, 0.1192, 0.035, 0.0279]\n","[Nov 10, 22:25:40] [0.1805, 0.1576, 0.1481, 0.1453, 0.1464, 0.1397, 0.135, 0.1227, 0.0855, 0.058]\n","[Nov 10, 22:25:40] [0.1886, 0.1729, 0.1482, 0.1397, 0.1401, 0.1377, 0.1349, 0.134, 0.1063, 0.058]\n","[Nov 10, 22:25:40] PATIENCE 0 / 10\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Nov 10, 22:26:01] TRAIN loss 1.500718953373196 19\n","[Nov 10, 22:26:02] VALIDATION F1micro, F1macro, loss: 0.33136094674556216 0.21563349781162464 507\n","[Nov 10, 22:26:02] PATIENCE 1 / 10\n","[Nov 10, 22:26:23] TRAIN loss 1.4949647902368426 20\n","[Nov 10, 22:26:25] VALIDATION F1micro, F1macro, loss: 0.39447731755424065 0.23302513830802618 507\n","[Nov 10, 22:26:25] PATIENCE 2 / 10\n","[Nov 10, 22:26:46] TRAIN loss 1.47785011390308 21\n","[Nov 10, 22:26:48] VALIDATION F1micro, F1macro, loss: 0.41025641025641024 0.24125015770768016 507\n","[Nov 10, 22:26:48] PATIENCE 3 / 10\n","[Nov 10, 22:27:08] TRAIN loss 1.461178274841996 22\n","[Nov 10, 22:27:10] VALIDATION F1micro, F1macro, loss: 0.42011834319526625 0.2343754333709173 507\n","[Nov 10, 22:27:10] PATIENCE 4 / 10\n","[Nov 10, 22:27:31] TRAIN loss 1.4545292567025434 23\n","[Nov 10, 22:27:33] VALIDATION F1micro, F1macro, loss: 0.3431952662721893 0.23580114068056926 507\n","[Nov 10, 22:27:33] PATIENCE 5 / 10\n","[Nov 10, 22:27:54] TRAIN loss 1.4422206637021657 24\n","[Nov 10, 22:27:55] VALIDATION F1micro, F1macro, loss: 0.39447731755424065 0.2221813223172231 507\n","[Nov 10, 22:27:55] PATIENCE 6 / 10\n","[Nov 10, 22:28:16] TRAIN loss 1.4302150319825422 25\n","[Nov 10, 22:28:18] VALIDATION F1micro, F1macro, loss: 0.4418145956607495 0.2640559708888115 507\n","[Nov 10, 22:28:18] PATIENCE 7 / 10\n","[Nov 10, 22:28:39] TRAIN loss 1.4064649622719567 26\n","[Nov 10, 22:28:41] VALIDATION F1micro, F1macro, loss: 0.46548323471400394 0.24790811409604685 507\n","[Nov 10, 22:28:41] PATIENCE 8 / 10\n","[Nov 10, 22:29:01] TRAIN loss 1.3841706127733797 27\n","[Nov 10, 22:29:03] VALIDATION F1micro, F1macro, loss: 0.5305719921104537 0.24942325838647994 507\n","[Nov 10, 22:29:03] PATIENCE 9 / 10\n","[Nov 10, 22:29:24] TRAIN loss 1.371157784182746 28\n","[Nov 10, 22:29:26] VALIDATION F1micro, F1macro, loss: 0.36489151873767256 0.22474214226803285 507\n","[Nov 10, 22:29:26] PATIENCE 10 / 10\n"]}],"source":["class vars():\n","    def __init__(self, mode):\n","        if mode == \"bow\":\n","            self.dataset = \"snes\"\n","            self.inputtype = \"CLAIM_AND_EVIDENCE\"\n","            self.filter_websites = 0\n","            self.model = \"bow\"\n","            self.batchsize = 2\n","            self.eval_per_epoch = 1\n","            self.lr = 0.0001\n","        elif mode == 'lstm':\n","            self.dataset = \"snes\"\n","            self.inputtype = \"CLAIM_AND_EVIDENCE\"\n","            self.filter_websites = 0\n","            self.model = \"lstm\"\n","            self.batchsize = 16\n","            self.eval_per_epoch = 1\n","            self.lr = 0.0001\n","            self.lstm_hidden_dim = 128\n","            self.lstm_layers = 2\n","            self.lstm_dropout = 0.1\n","        elif mode == 'bert':\n","            self.dataset = \"snes\"\n","            self.inputtype = \"CLAIM_AND_EVIDENCE\"\n","            self.filter_websites = 0\n","            self.model = \"bert\"\n","            self.batchsize = 8\n","            self.eval_per_epoch = 1\n","            self.lr = 0.000003            \n","\n","\n","            \n","args = vars(\"lstm\")\n","\n","if args.filter_websites > 0.5:\n","    savename = \"results/\" + \"-\".join([str(v) for v in [args.filter_websites, args.model, args.dataset, args.inputtype, args.lr, args.batchsize]])\n","else:\n","    savename = \"results/\" + \"-\".join([str(v) for v in [args.model, args.dataset, args.inputtype, args.lr, args.batchsize]])\n","\n","if args.model == \"lstm\":\n","    savename += \"-\" + \"-\".join([str(v) for v in [args.lstm_hidden_dim, args.lstm_layers, args.lstm_dropout]])\n","savename += \".pkl\"\n","\n","inputtype = INPUT_TYPE_ORDER.index(args.inputtype)\n","main_data, snippets_data, label_order, splits = load_data(args.dataset)\n","\n","if args.filter_websites > 0.5:\n","    snippets_data = filter_websites(snippets_data)\n","\n","params = {\"batch_size\": args.batchsize, \"shuffle\": True, \"num_workers\": 1, \"collate_fn\": transformer_collate, \"persistent_workers\": True, \"prefetch_factor\":5}\n","eval_params = {\"batch_size\": args.batchsize, \"shuffle\": False, \"num_workers\": 1, \"collate_fn\": transformer_collate, \"persistent_workers\": True, \"prefetch_factor\":5}\n","\n","train_generator, val_generator, test_generator, label_weights = make_generators(main_data, snippets_data, label_order, splits, [params, eval_params])\n","\n","if args.dataset == \"snes\":\n","    main_data, snippets_data, _, splits = load_data(\"pomt\")\n","    if args.filter_websites > 0.5:\n","        snippets_data = filter_websites(snippets_data)\n","    main_data.iloc[main_data.iloc[:, 2] == \"pants on fire!\", 2] = \"false\"\n","    main_data.iloc[main_data.iloc[:, 2] == \"half-true\", 2] = \"mixture\"\n","    _, _, other_test_generator, _ = make_generators(main_data, snippets_data, label_order, splits, [params, eval_params], other_dataset=True)\n","else:\n","    main_data, snippets_data, _, splits = load_data(\"snes\")\n","    if args.filter_websites > 0.5:\n","        snippets_data = filter_websites(snippets_data)\n","    main_data.iloc[main_data.iloc[:, 2] == \"mixture\", 2] = \"half-true\"\n","    _, _, other_test_generator, _ = make_generators(main_data, snippets_data, label_order, splits, [params, eval_params], other_dataset=True)\n","\n","\n","if args.model == \"bert\":\n","    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","    run_bert(args, train_generator, val_generator, test_generator, label_weights, inputtype, label_order, savename, other_test_generator)\n","elif args.model == \"lstm\":\n","    model = run_lstm(args, train_generator, val_generator, test_generator, label_weights, inputtype, label_order, savename, other_test_generator)\n","elif args.model == \"bow\":\n","    # print(\"run bow\")\n","    run_bow(args, train_generator, val_generator, test_generator, label_weights, inputtype, label_order, savename, other_test_generator)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":10,"id":"kbciz0o1c46y","metadata":{"executionInfo":{"elapsed":605,"status":"ok","timestamp":1636583367980,"user":{"displayName":"Benjamin Basseri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02068950276609094857"},"user_tz":300},"id":"kbciz0o1c46y"},"outputs":[],"source":["pickle.dump(model, open('lstm_claim_and_evidence', \"wb\"))"]},{"cell_type":"code","execution_count":12,"id":"9MbRdZHCc6Qa","metadata":{"executionInfo":{"elapsed":189,"status":"ok","timestamp":1636583858219,"user":{"displayName":"Benjamin Basseri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02068950276609094857"},"user_tz":300},"id":"9MbRdZHCc6Qa"},"outputs":[],"source":["def evaluate_after_train(generator, model, other_from=None, ignore_snippet=None):\n","    all_labels = []\n","    all_predictions = []\n","\n","    all_claimIDs = []\n","    all_logits = []\n","\n","    for vals in generator:\n","        claimIDs, claims, labels, snippets = vals[0], vals[1], vals[2], vals[3]\n","\n","        if ignore_snippet is not None:\n","            for i in range(len(snippets)):\n","                snippets[i][ignore_snippet] = \"filler\"\n","\n","        all_labels += labels\n","        logits = model(claims, snippets)\n","\n","        predictions = torch.argmax(logits, 1).cpu().numpy()\n","\n","        if other_from == \"pomt\": # other data is pomt, and model is trained on snes\n","            # this case is fine\n","            pass\n","        elif other_from == \"snes\": # other data is snes, and model is trained on pomt\n","            # in this case both \"pants on fire!\" and \"false\" should be considered as false\n","            predictions[predictions == 0] = 1 # 0 is \"pants on fire!\" and 1 is \"false\" for pomt.\n","\n","        all_predictions += predictions.tolist()\n","\n","        all_claimIDs += claimIDs\n","        all_logits += logits.detach().cpu().numpy().tolist()\n","\n","    f1_micro = f1_score(all_labels, all_predictions, average=\"micro\")\n","    f1_macro = f1_score(all_labels, all_predictions, average=\"macro\")\n","\n","    return f1_micro, f1_macro, all_claimIDs, all_logits, all_labels, all_predictions"]},{"cell_type":"code","execution_count":13,"id":"FoCokcyry1hE","metadata":{"executionInfo":{"elapsed":3463,"status":"ok","timestamp":1636583861895,"user":{"displayName":"Benjamin Basseri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02068950276609094857"},"user_tz":300},"id":"FoCokcyry1hE"},"outputs":[],"source":["#  returns f1_micro, f1_macro, all_claimIDs, all_logits, all_labels, all_predictions\n","f1_micro, f1_macro, _, _, labels, predictions = evaluate_after_train(test_generator, model, other_from=None, ignore_snippet=None)\n"]},{"cell_type":"code","execution_count":14,"id":"o2wcFEDs0f7Z","metadata":{"executionInfo":{"elapsed":199,"status":"ok","timestamp":1636583868561,"user":{"displayName":"Benjamin Basseri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02068950276609094857"},"user_tz":300},"id":"o2wcFEDs0f7Z"},"outputs":[],"source":["correct, incorrect = [], []\n","for i, (label, pred) in enumerate(zip(labels, predictions)):\n","    if label == pred:\n","        correct.append(i)\n","    else:\n","        incorrect.append(i)"]},{"cell_type":"code","execution_count":18,"id":"Op1OlFLjAAbm","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":177,"status":"ok","timestamp":1636583937168,"user":{"displayName":"Benjamin Basseri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02068950276609094857"},"user_tz":300},"id":"Op1OlFLjAAbm","outputId":"7d3acb3f-e422-41f7-989d-a3faf99e773a"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0, 5, 6, 7, 8]\n","[1, 2, 3, 4, 19]\n"]}],"source":["print(f\"{incorrect[:5]}\")\n","print(f\"{correct[:5]}\")"]},{"cell_type":"code","execution_count":27,"id":"Xn7cnmyDABUQ","metadata":{"executionInfo":{"elapsed":152,"status":"ok","timestamp":1636584423417,"user":{"displayName":"Benjamin Basseri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02068950276609094857"},"user_tz":300},"id":"Xn7cnmyDABUQ"},"outputs":[],"source":["dataset = \"snes\"\n","path = \"../multi_fc_publicdata/\" + dataset + \"/\"\n","splits = pickle.load(open(path + dataset + \"_index_split.pkl\", \"rb\"))\n","test_samples = splits[2]\n","\n","# Lookup table maps sample in dataset -> its index in the labels/predictions lists\n","sample_to_test_idx = {sample_idx: i for i, sample_idx in enumerate(test_samples)}\n","\n"]},{"cell_type":"code","execution_count":58,"id":"lHMYtQf2C-3B","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":149,"status":"ok","timestamp":1636585286916,"user":{"displayName":"Benjamin Basseri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02068950276609094857"},"user_tz":300},"id":"lHMYtQf2C-3B","outputId":"a68502b5-a375-47dc-8a0f-96b8e46fd589"},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","4696\n","1    Because of the failure to pass a repeal bill, ...\n","2                                              mixture\n","4    Chalk up Sen. David Perdue, R-Ga., as someone ...\n","Name: 4696, dtype: object\n","<class 'pandas.core.series.Series'>\n"]}],"source":["# First incorrect sample\n","print(incorrect[0])\n","\n","# Corresponds to index:\n","print(test_samples[incorrect[0]])\n","cols = [1, 2, 4] # headline, label, text\n","\n","# Observe this sample\n","print(main_data.iloc[4696, cols])\n","obs = main_data.iloc[4696, cols]\n","print(type(obs))"]},{"cell_type":"code","execution_count":62,"id":"-TFiimcmEGSC","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":175,"status":"ok","timestamp":1636585436769,"user":{"displayName":"Benjamin Basseri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02068950276609094857"},"user_tz":300},"id":"-TFiimcmEGSC","outputId":"5b9f2a6c-5bf0-49d5-82b7-34814da8f44c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Headline: Because of the failure to pass a repeal bill, \"Obamacare remains the law of the land ... This means more than 300,000 Georgians below the poverty line will still not have access to the insurance Obamacare promised.\"\n","Label: mixture\n","Text: Chalk up Sen. David Perdue, R-Ga., as someone who was unhappy about the Senate’s failure to pass a bill to repeal and replace the Affordable Care Act. After the Senate fell one vote short of overturning key elements of President Barack Obama’s signature health care law, Perdue offered a statement, reprinted here in part: \"Throughout this entire process, we have witnessed everything that’s wrong with Washington. The Senate had a real opportunity to dismantle the most damaging parts of Obamacare. As Republicans have railed against the failures of Obamacare for the last seven years, Democrats have failed to acknowledge any shortcomings of Obamacare and refused to try to fix a broken system. \"Now, due to an unworkable budget process and politicians who put their political self-interests ahead of national interest, Obamacare remains the law of the land. This means that the poorest among us will continue to be fined because they can’t afford Obamacare. This means more than 300,000 Georgians below the poverty line will still not have access to the insurance Obamacare promised. Additionally, this means the Medicaid expansion will continue as an open-ended government program that cannot be sustained.\" We wondered whether Perdue was right that the failure to pass the repeal bill means that \"more than 300,000 Georgians below the poverty line will still not have access to the insurance Obamacare promised.\" Perdue’s office told PolitiFact that he correctly stated that even with Obamacare in place, 300,000 Georgians below the poverty line can’t get insurance. The data we found showed that to be the case. However, in decrying the demise of the GOP bills, Perdue glosses over the fact that independent experts say the Republican replacement bills would not have solved this problem; in fact, they would have made it worse. Senate bill tax credits According to the Kaiser Family Foundation, 309,000 people in Georgia are in the \"coverage gap\" -- a no-man’s-land where their income puts them above current Medicaid eligibility for their state but below the lower limit for premium tax credits for plans purchased on the ACA’s online marketplaces. The House and Senate bills would have addressed this, but only in a limited way. \"Both the House and Senate bill would have offered these individuals tax credits,\" said Christine Eibner, a senior economist at the RAND Corp. But while these tax credits are \"arguably better than nothing,\" Eibner said, it’s not really a viable policy solution for insuring hundreds of thousands of additional Georgians. That’s because the policies available would have such large deductibles that they’d either offer little protection from health care costs or scare off customers from buying a plan in the first place. While the tax credits in the Senate bill would have required relatively small premiums for those in poverty, the independent Congressional Budget Office estimated that the deductible for a typical plan under the Senate bill would be $6,000, likely making it \"unaffordable for a person with income under the poverty level,\" Eibner said. And for the House bill, Eibner said, the CBO estimated that a low-income older person \"could face net premiums in the range of $13,600 to $16,100, even after accounting the tax credits available.\" Such realities played into the CBO’s conclusion that the Senate bill nationally would leave 22 million fewer Americans insured by 2026 than if current law continued. The CBO’s estimate for the House bill was 23 million fewer Americans insured. As for Georgia specifically, the Senate bill would have increased the number of uninsured Georgians by 376,000 in 2022, according to an analysis by the Urban Institute. For the House bill, the Urban Institute projected an additional 304,000 uninsured Georgians in 2022. Bottom line: Those Georgians might have \"access\" to policies, as Purdue put it, but that’s a far cry from actually getting insurance, much less adequate insurance. The reality, independent experts say, is that the coverage they’d have access to doesn’t offer out-of-pocket costs that are affordable enough to get many new people on the insurance rolls. In fact, any of the repeal-and-replace proposals would have decreased the number of people with insurance substantially compared to current law. No state Medicaid expansion There’s another problem: Georgia turned down an opportunity under the Affordable Care Act to expand Georgia’s eligibility for Medicaid. The state would have had to pick up about 10 percent of the bill, with the federal government paying about 90 percent. The most recent analysis by the Urban Institute found that Medicaid expansion in Georgia -- which is still on the table if the GOP-led state wanted to accept it -- would reduce the number of uninsured Georgians by between 460,000 and 556,000 people in 2021. In other words, the state could have covered even more low-income Georgians than Perdue’s 300,000 by expanding Medicaid under Obamacare. (And, as we noted above, it’s unlikely that the Republican bills would have covered those 300,000.) \"To blame the ACA for the state government of Georgia’s decision not to expand Medicaid eligibility is a tremendous distortion,\" said Urban Institute health policy specialist Linda Blumberg. Our ruling Perdue said that because of the failure to pass a repeal bill, \"Obamacare remains the law of the land ... This means more than 300,000 Georgians below the poverty line will still not have access to the insurance Obamacare promised.\" Perdue has a point: Even with Obamacare in place, 300,000 Georgians below the poverty line can’t get insurance. However, in the context of decrying the failure of the Republican bills to advance in Congress, this observation is misleading. While the Senate and House bills would have offered tax credits to low-income Americans, independent analysts agree that these tax credits wouldn’t open the door to affordable insurance for low-income Americans. Perdue’s criticism of Obamacare is also problematic. In reality, the ACA’s Medicaid expansion could have covered even more than 300,000 low-income Georgians had it not been rejected by Perdue’s fellow Republicans in Georgia. We rate the statement Half True. See Figure 1 on PolitiFact.com\n"]}],"source":["values = obs.values\n","print(f\"Headline: {values[0]}\")\n","print(f\"Label: {values[1]}\")\n","print(f\"Text: {values[2]}\")"]},{"cell_type":"code","execution_count":77,"id":"B1RBUmTsBfJO","metadata":{"executionInfo":{"elapsed":151,"status":"ok","timestamp":1636586475835,"user":{"displayName":"Benjamin Basseri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02068950276609094857"},"user_tz":300},"id":"B1RBUmTsBfJO"},"outputs":[],"source":["import textwrap\n","\n","def write_samples_to_file(data, indices, filename):\n","    \"\"\"indices: the label/prediction indicies you want the corresponding sample for\"\"\"\n","    cols = [1, 2, 4]\n","    samples = [test_samples[i] for i in indices]\n","    data_wanted = data.iloc[samples, cols]\n","    with open(filename, 'w') as f:\n","        for i, row in data_wanted.iterrows():\n","            headline, label, text = row.values[0], row.values[1], row.values[2]\n","            headline_lines = textwrap.wrap(headline, width=80)\n","            lines = textwrap.wrap(text, width=80)\n","            f.write(\"HEADLINE:\\n\")\n","            for line in headline_lines:\n","                f.write(line + '\\n')\n","            f.write(\"LABEL: \" + label + '\\n')\n","            f.write(\"TEXT: \")\n","            for line in lines:\n","                f.write(line + '\\n')\n","            f.write(\"===============================\\n\")\n","\n","    "]},{"cell_type":"code","execution_count":79,"id":"aFZEdhGQCf-d","metadata":{"executionInfo":{"elapsed":2075,"status":"ok","timestamp":1636586509189,"user":{"displayName":"Benjamin Basseri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02068950276609094857"},"user_tz":300},"id":"aFZEdhGQCf-d"},"outputs":[],"source":["write_samples_to_file(main_data, correct, 'correct.txt')\n","write_samples_to_file(main_data, incorrect, 'incorrect.txt')"]},{"cell_type":"code","execution_count":105,"id":"vc1h7BrAe0OT","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":177,"status":"ok","timestamp":1636593618246,"user":{"displayName":"Benjamin Basseri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02068950276609094857"},"user_tz":300},"id":"vc1h7BrAe0OT","outputId":"81a6b82b-bfb2-4e51-c8da-7dc40e687a94"},"outputs":[{"data":{"text/plain":["[32, 35, 36, 38, 40]"]},"execution_count":105,"metadata":{},"output_type":"execute_result"}],"source":["correct[10:15]"]},{"cell_type":"code","execution_count":107,"id":"FlOd8Z5JfYWE","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":183,"status":"ok","timestamp":1636593819628,"user":{"displayName":"Benjamin Basseri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02068950276609094857"},"user_tz":300},"id":"FlOd8Z5JfYWE","outputId":"35bea693-ea25-4a9d-9013-9f80edfcf83f"},"outputs":[{"data":{"text/plain":["0                                            pomt-12218\n","1     Kim Jong Un dead: North Koreans calling Trump ...\n","2                                                 false\n","3     /punditfact/statements/2017/jul/20/blog-postin...\n","4     A report that Kim Jong Un is dead and that it ...\n","5                                                  None\n","6                                              Bloggers\n","7                                                  None\n","8                                                  None\n","9                                                  None\n","10                                  2017-07-20T17:25:19\n","11                                           2017-07-06\n","12                                             ['None']\n","Name: 2750, dtype: object"]},"execution_count":107,"metadata":{},"output_type":"execute_result"}],"source":["main_data.loc[test_samples[35], :]"]},{"cell_type":"code","execution_count":null,"id":"qloIZryffdTb","metadata":{"id":"qloIZryffdTb"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"main_copy.ipynb","provenance":[]},"kernelspec":{"display_name":"Python [conda env:sk] *","language":"python","name":"conda-env-sk-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":5}
